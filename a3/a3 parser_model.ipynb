{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6uANNwGXppJrtmzk/zukS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"dqC8dExrMlG1","executionInfo":{"status":"ok","timestamp":1718876448147,"user_tz":-330,"elapsed":8119,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}}},"outputs":[],"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2021-2022: Homework 3\n","parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n","Sahil Chopra <schopra8@stanford.edu>\n","Haoshen Hong <haoshen@stanford.edu>\n","\"\"\"\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ParserModel(nn.Module):\n","    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n","    The ParserModel will predict which transition should be applied to a\n","    given partial parse configuration.\n","\n","    PyTorch Notes:\n","        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n","            are a subclass of this \"nn.Module\".\n","        - The \"__init__\" method is where you define all the layers and parameters\n","            (embedding layers, linear layers, dropout layers, etc.).\n","        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n","            when you write \"m = ParserModel()\".\n","        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n","            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n","            in other ParserModel methods.\n","        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n","    \"\"\"\n","    def __init__(self, embeddings, n_features=36,\n","        hidden_size=200, n_classes=3, dropout_prob=0.5):\n","        \"\"\" Initialize the parser model.\n","\n","        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n","        @param n_features (int): number of input features\n","        @param hidden_size (int): number of hidden units\n","        @param n_classes (int): number of output classes\n","        @param dropout_prob (float): dropout probability\n","        \"\"\"\n","        super(ParserModel, self).__init__()\n","        self.n_features = n_features\n","        self.n_classes = n_classes\n","        self.dropout_prob = dropout_prob\n","        self.embed_size = embeddings.shape[1]\n","        self.hidden_size = hidden_size\n","        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n","\n","        self.logit_size = self.n_classes\n","\n","        ### YOUR CODE HERE (~9-10 Lines)\n","        ### TODO:\n","        ###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`.\n","        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n","        ###        with default parameters.\n","        ###     2) Construct `self.dropout` layer.\n","        ###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.\n","        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n","        ###        with default parameters.\n","        ###\n","        ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API\n","        ###       to include a tensor into a computational graph to support updating w.r.t its gradient.\n","        ###       Here, we use Xavier Uniform Initialization for our Weight initialization.\n","        ###       It has been shown empirically, that this provides better initial weights\n","        ###       for training networks than random uniform initialization.\n","        ###       For more details checkout this great blogpost:\n","        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n","        ###\n","        ### Please see the following docs for support:\n","        ###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters\n","        ###     Initialization: https://pytorch.org/docs/stable/nn.init.html\n","        ###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n","        ###\n","        ### See the PDF for hints.\n","\n","        #parameters\n","        self.embed_to_hidden_weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.embed_size * self.n_features, self.hidden_size)))\n","        self.embed_to_hidden_bias = nn.Parameter(nn.init.uniform_(torch.empty(self.hidden_size)))\n","        self.hidden_to_logits_weight = nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.hidden_size, self.logit_size)))\n","        self.hidden_to_logits_bias = nn.Parameter(nn.init.uniform_(torch.empty(self.logit_size)))\n","\n","        #layers\n","        self.dropoutLayer = torch.nn.Dropout(p=self.dropout_prob, inplace=False)\n","        self.relu = nn.ReLU()\n","\n","\n","        ### END YOUR CODE\n","\n","    def embedding_lookup(self, w):\n","        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n","            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n","\n","            @return x (Tensor): tensor of embeddings for words represented in w\n","                                (batch_size, n_features * embed_size)\n","        \"\"\"\n","\n","        ### YOUR CODE HERE (~1-4 Lines)\n","        ### TODO:\n","        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n","        ###     2) Reshape the tensor using `view` function if necessary\n","        ###\n","        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives\n","        ###       a list of indices representing a sequence of words, then it calls this lookup\n","        ###       function to map indices to sequence of embeddings.\n","        ###\n","        ###       This problem aims to test your understanding of embedding lookup,\n","        ###       so DO NOT use any high level API like nn.Embedding\n","        ###       (we are asking you to implement that!). Pay attention to tensor shapes\n","        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!\n","        ###\n","        ### Pytorch has some useful APIs for you, and you can use either one\n","        ### in this problem (except nn.Embedding). These docs might be helpful:\n","        ###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select\n","        ###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather\n","        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n","        ###     Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html\n","\n","        # x = []\n","        # for ele in w:\n","        #   w_embedding_2d = torch.index_select(self.embeddings, 0, ele)\n","        #   w_embedding_1d = torch.flatten(w_embedding_2d)\n","        #   x.append(w_embedding_1d)\n","\n","        # x = torch.stack(x, dim=0)\n","\n","        x = torch.index_select(self.embeddings, 0, w.flatten()).view(w.size(0), -1)\n","\n","        ### END YOUR CODE\n","        return x\n","\n","\n","    def forward(self, w):\n","        \"\"\" Run the model forward.\n","\n","            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n","\n","            PyTorch Notes:\n","                - Every nn.Module object (PyTorch model) has a `forward` function.\n","                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n","                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n","                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n","                        model = ParserModel()\n","                        output = model(w) # this calls the forward function\n","                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n","\n","        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n","\n","        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n","                                 without applying softmax (batch_size, n_classes)\n","        \"\"\"\n","        ### YOUR CODE HERE (~3-5 lines)\n","        ### TODO:\n","        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer\n","        ###     as decleared in `__init__` after ReLU function.\n","        ###\n","        ### Note: We do not apply the softmax to the logits here, because\n","        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n","        ###\n","        ### Please see the following docs for support:\n","        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul\n","        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n","\n","        x = self.embedding_lookup(w)\n","        hidden = x @ self.embed_to_hidden_weight + self.embed_to_hidden_bias\n","        relu = self.relu(hidden)\n","        dropped = self.dropoutLayer(relu)\n","        logits = dropped @ self.hidden_to_logits_weight + self.hidden_to_logits_bias\n","\n","\n","\n","        ### END YOUR CODE\n","        return logits\n","\n","\n","# if __name__ == \"__main__\":\n","\n","#     parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n","#     parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n","#     parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n","#     args = parser.parse_args()\n","\n","#     embeddings = np.zeros((100, 30), dtype=np.float32)\n","#     model = ParserModel(embeddings)\n","\n","#     def check_embedding():\n","#         inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n","#         selected = model.embedding_lookup(inds)\n","#         assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n","#                                       + repr(selected) + \" contains non-zero elements.\"\n","\n","#     def check_forward():\n","#         inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n","#         out = model(inputs)\n","#         expected_out_shape = (4, 3)\n","#         assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n","#                                                 \" which doesn't match expected \" + repr(expected_out_shape)\n","\n","#     if args.embedding:\n","#         check_embedding()\n","#         print(\"Embedding_lookup sanity check passes!\")\n","\n","#     if args.forward:\n","#         check_forward()\n","#         print(\"Forward sanity check passes!\")"]},{"cell_type":"code","source":["embeddings = np.zeros((100, 30), dtype=np.float32)\n","model = ParserModel(embeddings)\n","\n","def check_embedding():\n","    inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n","    selected = model.embedding_lookup(inds)\n","    assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n","                                  + repr(selected) + \" contains non-zero elements.\"\n","\n","def check_forward():\n","    inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n","    out = model(inputs)\n","    expected_out_shape = (4, 3)\n","    assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n","                                            \" which doesn't match expected \" + repr(expected_out_shape)\n"],"metadata":{"id":"PKTFD3PiNgWA","executionInfo":{"status":"ok","timestamp":1718876448148,"user_tz":-330,"elapsed":12,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Embedding"],"metadata":{"id":"GGUT5tQcNvzL"}},{"cell_type":"code","source":["check_embedding()\n","print(\"Embedding_lookup sanity check passes!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkXxIaedNxhq","executionInfo":{"status":"ok","timestamp":1718876448149,"user_tz":-330,"elapsed":11,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}},"outputId":"e2d5d735-6a3e-4578-f69c-e70827eb0bc1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding_lookup sanity check passes!\n"]}]},{"cell_type":"markdown","source":["# Forward"],"metadata":{"id":"3BKzZomcNzVK"}},{"cell_type":"code","source":["check_forward()\n","print(\"Forward sanity check passes!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkFWOsAFN1cx","executionInfo":{"status":"ok","timestamp":1718876448149,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}},"outputId":"25aac375-b9c0-4519-cf74-0dc68aeaddb5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Forward sanity check passes!\n"]}]}]}