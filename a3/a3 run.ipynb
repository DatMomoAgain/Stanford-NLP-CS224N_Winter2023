{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8Z1KeFIEiqz70QxxbVGrR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hlCJ0GwcpIQE","executionInfo":{"status":"ok","timestamp":1718879748054,"user_tz":-330,"elapsed":4049,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}},"outputId":"a866cd7a-9527-4aef-a88b-43faa5480b1e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","import sys\n","sys.path.append('/content/gdrive/My Drive/stanford-nlp/a3')\n","os.chdir(\"/content/gdrive/MyDrive/stanford-nlp/a3\")\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NvLZln7ppQIK","executionInfo":{"status":"ok","timestamp":1718879748054,"user_tz":-330,"elapsed":14,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}},"outputId":"b4c5d267-4617-429d-f494-08abc14d6621"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["collect_submission.sh  local_env.yml\tparser_transitions.py  README.txt  run.py\n","data\t\t       parser_model.py\t__pycache__\t       results\t   utils\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","CS224N 2021-2022: Homework 3\n","run.py: Run the dependency parser.\n","Sahil Chopra <schopra8@stanford.edu>\n","Haoshen Hong <haoshen@stanford.edu>\n","\"\"\"\n","from datetime import datetime\n","import os\n","import pickle\n","import math\n","import time\n","import argparse\n","\n","from torch import nn, optim\n","import torch\n","from tqdm import tqdm\n","\n","from parser_model import ParserModel\n","from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n","\n","# parser = argparse.ArgumentParser(description='Train neural dependency parser in pytorch')\n","# parser.add_argument('-d', '--debug', action='store_true', help='whether to enter debug mode')\n","# args = parser.parse_args()\n","\n","# -----------------\n","# Primary Functions\n","# -----------------\n","def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0006):\n","    \"\"\" Train the neural dependency parser.\n","\n","    @param parser (Parser): Neural Dependency Parser\n","    @param train_data ():\n","    @param dev_data ():\n","    @param output_path (str): Path to which model weights and results are written.\n","    @param batch_size (int): Number of examples in a single batch\n","    @param n_epochs (int): Number of training epochs\n","    @param lr (float): Learning rate\n","    \"\"\"\n","    best_dev_UAS = 0\n","\n","\n","    ### YOUR CODE HERE (~2-7 lines)\n","    ### TODO:\n","    ###      1) Construct Adam Optimizer in variable `optimizer`\n","    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n","    ###         reduction (default)\n","    ###\n","    ### Hint: Use `parser.model.parameters()` to pass optimizer\n","    ###       necessary parameters to tune.\n","    ### Please see the following docs for support:\n","    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n","    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n","\n","    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n","    loss_func = nn.CrossEntropyLoss()\n","\n","    ### END YOUR CODE\n","\n","    for epoch in range(n_epochs):\n","        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n","        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n","        if dev_UAS > best_dev_UAS:\n","            best_dev_UAS = dev_UAS\n","            print(\"New best dev UAS! Saving model.\")\n","            torch.save(parser.model.state_dict(), output_path)\n","        print(\"\")\n","\n","\n","def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n","    \"\"\" Train the neural dependency parser for single epoch.\n","\n","    Note: In PyTorch we can signify train versus test and automatically have\n","    the Dropout Layer applied and removed, accordingly, by specifying\n","    whether we are training, `model.train()`, or evaluating, `model.eval()`\n","\n","    @param parser (Parser): Neural Dependency Parser\n","    @param train_data ():\n","    @param dev_data ():\n","    @param optimizer (nn.Optimizer): Adam Optimizer\n","    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n","    @param batch_size (int): batch size\n","\n","    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n","    \"\"\"\n","    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n","    n_minibatches = math.ceil(len(train_data) / batch_size)\n","    loss_meter = AverageMeter()\n","\n","    with tqdm(total=(n_minibatches)) as prog:\n","        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n","            optimizer.zero_grad()   # remove any baggage in the optimizer\n","            loss = 0. # store loss for this batch herewithin\n","            train_x = torch.from_numpy(train_x).long()\n","            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n","\n","            ### YOUR CODE HERE (~4-10 lines)\n","            ### TODO:\n","            ###      1) Run train_x forward through model to produce `logits`\n","            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n","            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n","            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n","            ###         are the predictions (y^ from the PDF).\n","            ###      3) Backprop losses\n","            ###      4) Take step with the optimizer\n","            ### Please see the following docs for support:\n","            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n","\n","            logits = parser.model(train_x)\n","            loss = loss_func(logits, train_y)\n","            loss.backward()\n","            optimizer.step()\n","\n","            ### END YOUR CODE\n","            prog.update(1)\n","            loss_meter.update(loss.item())\n","\n","    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n","\n","    print(\"Evaluating on dev set\",)\n","    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n","    dev_UAS, _ = parser.parse(dev_data)\n","    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n","    return dev_UAS\n","\n","\n","#if __name__ == \"__main__\":\n","    # debug = args.debug\n","\n","    # assert (torch.__version__.split(\".\") >= [\"1\", \"0\", \"0\"]), \"Please install torch version >= 1.0.0\"\n","\n","    # print(80 * \"=\")\n","    # print(\"INITIALIZING\")\n","    # print(80 * \"=\")\n","    # parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n","\n","    # start = time.time()\n","    # model = ParserModel(embeddings, hidden_size=256, dropout_prob=0.3)\n","    # parser.model = model\n","    # print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n","\n","    # print(80 * \"=\")\n","    # print(\"TRAINING\")\n","    # print(80 * \"=\")\n","    # output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n","    # output_path = output_dir + \"model.weights\"\n","\n","    # if not os.path.exists(output_dir):\n","    #     os.makedirs(output_dir)\n","\n","    # train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n","\n","    # if not debug:\n","    #     print(80 * \"=\")\n","    #     print(\"TESTING\")\n","    #     print(80 * \"=\")\n","    #     print(\"Restoring the best model weights found on the dev set\")\n","    #     parser.model.load_state_dict(torch.load(output_path))\n","    #     print(\"Final evaluation on test set\",)\n","    #     parser.model.eval()\n","    #     UAS, dependencies = parser.parse(test_data)\n","    #     print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n","    #     print(\"Done!\")"],"metadata":{"id":"w4L0p39KpSG5","executionInfo":{"status":"ok","timestamp":1718879748054,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# debug = args.debug\n","debug = False\n","\n","assert (torch.__version__.split(\".\") >= [\"1\", \"0\", \"0\"]), \"Please install torch version >= 1.0.0\"\n","\n","print(80 * \"=\")\n","print(\"INITIALIZING\")\n","print(80 * \"=\")\n","parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n","\n","start = time.time()\n","model = ParserModel(embeddings, hidden_size=256, dropout_prob=0.3)\n","parser.model = model\n","print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n","\n","print(80 * \"=\")\n","print(\"TRAINING\")\n","print(80 * \"=\")\n","output_dir = \"results/{:%Y.%m.%d_%H:%M:%S}/\".format(datetime.now())\n","output_path = output_dir + \"model.weights\"\n","\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","\n","train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0006)\n","\n","if not debug:\n","  print(80 * \"=\")\n","  print(\"TESTING\")\n","  print(80 * \"=\")\n","  print(\"Restoring the best model weights found on the dev set\")\n","  parser.model.load_state_dict(torch.load(output_path))\n","  print(\"Final evaluation on test set\",)\n","  parser.model.eval()\n","  UAS, dependencies = parser.parse(test_data)\n","  print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n","  print(\"Done!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rz_e9Yj_pwAB","outputId":"7b2af398-c16b-4a68-f828-7a36b610c263","executionInfo":{"status":"ok","timestamp":1718881241288,"user_tz":-330,"elapsed":1493243,"user":{"displayName":"Mohak Khetan","userId":"10308887064164029680"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","INITIALIZING\n","================================================================================\n","Loading data...\n","took 2.30 seconds\n","Building parser...\n","took 2.13 seconds\n","Loading pretrained embeddings...\n","took 3.16 seconds\n","Vectorizing data...\n","took 1.38 seconds\n","Preprocessing training data...\n","took 52.15 seconds\n","took 0.01 seconds\n","\n","================================================================================\n","TRAINING\n","================================================================================\n","Epoch 1 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:21<00:00, 13.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.16808760447177665\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 26447393.52it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 85.14\n","New best dev UAS! Saving model.\n","\n","Epoch 2 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:17<00:00, 13.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.09798737496411775\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 36332742.83it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 87.14\n","New best dev UAS! Saving model.\n","\n","Epoch 3 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:17<00:00, 13.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.08408330067250377\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 37570995.84it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 88.19\n","New best dev UAS! Saving model.\n","\n","Epoch 4 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:17<00:00, 13.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.0749703907544898\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 35156784.81it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 88.25\n","New best dev UAS! Saving model.\n","\n","Epoch 5 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:21<00:00, 13.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.06791776689616116\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 36906087.21it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 88.99\n","New best dev UAS! Saving model.\n","\n","Epoch 6 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:17<00:00, 13.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.06209659648180266\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 35233995.90it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 89.39\n","New best dev UAS! Saving model.\n","\n","Epoch 7 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:18<00:00, 13.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.05717202987183224\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 34419874.56it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 88.83\n","\n","Epoch 8 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:18<00:00, 13.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.05274030378842283\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 34205926.13it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 89.03\n","\n","Epoch 9 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:17<00:00, 13.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.04901887655250115\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 34909274.50it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 89.07\n","\n","Epoch 10 out of 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1848/1848 [02:18<00:00, 13.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average Train Loss: 0.045506820811394164\n","Evaluating on dev set\n"]},{"output_type":"stream","name":"stderr","text":["1445850it [00:00, 36720604.78it/s]      \n"]},{"output_type":"stream","name":"stdout","text":["- dev UAS: 89.28\n","\n","================================================================================\n","TESTING\n","================================================================================\n","Restoring the best model weights found on the dev set\n","Final evaluation on test set\n"]},{"output_type":"stream","name":"stderr","text":["2919736it [00:00, 53269160.50it/s]      "]},{"output_type":"stream","name":"stdout","text":["- test UAS: 89.69\n","Done!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}